{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "language_mapping = {\n",
    "    \"TGLANG_LANGUAGE_OTHER\": \"other\",\n",
    "    \"TGLANG_LANGUAGE_1S_ENTERPRISE\": \"TGLANG_LANGUAGE_1S_ENTERPRISE\",\n",
    "    \"TGLANG_LANGUAGE_ABAP\": \"abap\",\n",
    "    \"TGLANG_LANGUAGE_ACTIONSCRIPT\": \"actionscript\",\n",
    "    \"TGLANG_LANGUAGE_ADA\": \"ada\",\n",
    "    \"TGLANG_LANGUAGE_APACHE_GROOVY\": \"groovy\",\n",
    "    \"TGLANG_LANGUAGE_APEX\": \"TGLANG_LANGUAGE_APEX\",\n",
    "    \"TGLANG_LANGUAGE_APPLESCRIPT\": \"applescript\",\n",
    "    \"TGLANG_LANGUAGE_ASP\": \"asp\",\n",
    "    \"TGLANG_LANGUAGE_ASSEMBLY\": \"assembly\",\n",
    "    \"TGLANG_LANGUAGE_AUTOHOTKEY\": \"autohotkey\",\n",
    "    \"TGLANG_LANGUAGE_AWK\": \"awk\",\n",
    "    \"TGLANG_LANGUAGE_BASIC\": \"TGLANG_LANGUAGE_BASIC\",\n",
    "    \"TGLANG_LANGUAGE_BATCH\": \"batchfile\",\n",
    "    \"TGLANG_LANGUAGE_BISON\": \"bison\",\n",
    "    \"TGLANG_LANGUAGE_C\": \"c\",\n",
    "    \"TGLANG_LANGUAGE_CLOJURE\": \"clojure\",\n",
    "    \"TGLANG_LANGUAGE_CMAKE\": \"cmake\",\n",
    "    \"TGLANG_LANGUAGE_COBOL\": \"cobol\",\n",
    "    \"TGLANG_LANGUAGE_COFFESCRIPT\": \"coffeescript\",\n",
    "    \"TGLANG_LANGUAGE_COMMON_LISP\": \"common-lisp\",\n",
    "    \"TGLANG_LANGUAGE_CPLUSPLUS\": \"c++\",\n",
    "    \"TGLANG_LANGUAGE_CRYSTAL\": \"crystal\",\n",
    "    \"TGLANG_LANGUAGE_CSHARP\": \"c-sharp\",\n",
    "    \"TGLANG_LANGUAGE_CSS\": \"css\",\n",
    "    \"TGLANG_LANGUAGE_CSV\": \"csv\",\n",
    "    \"TGLANG_LANGUAGE_D\": \"d\",\n",
    "    \"TGLANG_LANGUAGE_DART\": \"dart\",\n",
    "    \"TGLANG_LANGUAGE_DELPHI\": \"TGLANG_LANGUAGE_DELPHI\",\n",
    "    \"TGLANG_LANGUAGE_DOCKER\": \"dockerfile\",\n",
    "    \"TGLANG_LANGUAGE_ELIXIR\": \"elixir\",\n",
    "    \"TGLANG_LANGUAGE_ELM\": \"elm\",\n",
    "    \"TGLANG_LANGUAGE_ERLANG\": \"erlang\",\n",
    "    \"TGLANG_LANGUAGE_FIFT\": \"FIFT\",\n",
    "    \"TGLANG_LANGUAGE_FORTH\": \"forth\",\n",
    "    \"TGLANG_LANGUAGE_FORTRAN\": \"fortran\",\n",
    "    \"TGLANG_LANGUAGE_FSHARP\": \"f-sharp\",\n",
    "    \"TGLANG_LANGUAGE_FUNC\": \"FUNC\",\n",
    "    \"TGLANG_LANGUAGE_GAMS\": \"gams\",\n",
    "    \"TGLANG_LANGUAGE_GO\": \"go\",\n",
    "    \"TGLANG_LANGUAGE_GRADLE\": \"TGLANG_LANGUAGE_GRADLE\",\n",
    "    \"TGLANG_LANGUAGE_GRAPHQL\": \"graphql\",\n",
    "    \"TGLANG_LANGUAGE_HACK\": \"TGLANG_LANGUAGE_HACK\",\n",
    "    \"TGLANG_LANGUAGE_HASKELL\": \"haskell\",\n",
    "    \"TGLANG_LANGUAGE_HTML\": \"html\",\n",
    "    \"TGLANG_LANGUAGE_ICON\": \"TGLANG_LANGUAGE_ICON\",\n",
    "    \"TGLANG_LANGUAGE_IDL\": \"idl\",\n",
    "    \"TGLANG_LANGUAGE_INI\": \"ini\",\n",
    "    \"TGLANG_LANGUAGE_JAVA\": \"java\",\n",
    "    \"TGLANG_LANGUAGE_JAVASCRIPT\": \"javascript\",\n",
    "    \"TGLANG_LANGUAGE_JSON\": \"TGLANG_LANGUAGE_JSON\",\n",
    "    \"TGLANG_LANGUAGE_JULIA\": \"julia\",\n",
    "    \"TGLANG_LANGUAGE_KEYMAN\": \"kmn_dataset\",\n",
    "    \"TGLANG_LANGUAGE_KOTLIN\": \"kotlin\",\n",
    "    \"TGLANG_LANGUAGE_LATEX\": \"TGLANG_LANGUAGE_LATEX\",\n",
    "    \"TGLANG_LANGUAGE_LISP\": \"TGLANG_LANGUAGE_LISP\",\n",
    "    \"TGLANG_LANGUAGE_LOGO\": \"TGLANG_LANGUAGE_LOGO\",\n",
    "    \"TGLANG_LANGUAGE_LUA\": \"lua\",\n",
    "    \"TGLANG_LANGUAGE_MAKEFILE\": \"makefile\",\n",
    "    \"TGLANG_LANGUAGE_MARKDOWN\": \"markdown\",\n",
    "    \"TGLANG_LANGUAGE_MATLAB\": \"matlab\",\n",
    "    \"TGLANG_LANGUAGE_NGINX\": \"nginx\",\n",
    "    \"TGLANG_LANGUAGE_NIM\": \"TGLANG_LANGUAGE_NIM\",\n",
    "    \"TGLANG_LANGUAGE_OBJECTIVE_C\": \"TGLANG_LANGUAGE_OBJECTIVE_C\",\n",
    "    \"TGLANG_LANGUAGE_OCAML\": \"ocaml\",\n",
    "    \"TGLANG_LANGUAGE_OPENEDGE_ABL\": \"TGLANG_LANGUAGE_OPENEDGE_ABL\",\n",
    "    \"TGLANG_LANGUAGE_PASCAL\": \"pascal\",\n",
    "    \"TGLANG_LANGUAGE_PERL\": \"perl\",\n",
    "    \"TGLANG_LANGUAGE_PHP\": \"php\",\n",
    "    \"TGLANG_LANGUAGE_PL_SQL\": \"TGLANG_LANGUAGE_PL_SQL\",\n",
    "    \"TGLANG_LANGUAGE_POWERSHELL\": \"powershell\",\n",
    "    \"TGLANG_LANGUAGE_PROLOG\": \"prolog\",\n",
    "    \"TGLANG_LANGUAGE_PROTOBUF\": \"TGLANG_LANGUAGE_PROTOBUF\",\n",
    "    \"TGLANG_LANGUAGE_PYTHON\": \"python\",\n",
    "    \"TGLANG_LANGUAGE_QML\": \"qml\",\n",
    "    \"TGLANG_LANGUAGE_R\": \"r\",\n",
    "    \"TGLANG_LANGUAGE_RAKU\": \"TGLANG_LANGUAGE_RAKU\",\n",
    "    \"TGLANG_LANGUAGE_REGEX\": \"regexp\",\n",
    "    \"TGLANG_LANGUAGE_RUBY\": \"ruby\",\n",
    "    \"TGLANG_LANGUAGE_RUST\": \"rust\",\n",
    "    \"TGLANG_LANGUAGE_SAS\": \"sas\",\n",
    "    \"TGLANG_LANGUAGE_SCALA\": \"scala\",\n",
    "    \"TGLANG_LANGUAGE_SCHEME\": \"scheme\",\n",
    "    \"TGLANG_LANGUAGE_SHELL\": \"shell\",\n",
    "    \"TGLANG_LANGUAGE_SMALLTALK\": \"smalltalk\",\n",
    "    \"TGLANG_LANGUAGE_SOLIDITY\": \"solidity\",\n",
    "    \"TGLANG_LANGUAGE_SQL\": \"sql\",\n",
    "    \"TGLANG_LANGUAGE_SWIFT\": \"swift\",\n",
    "    \"TGLANG_LANGUAGE_TCL\": \"tcl\",\n",
    "    \"TGLANG_LANGUAGE_TEXTILE\": \"textile\",\n",
    "    \"TGLANG_LANGUAGE_TL\": \"thrift\",\n",
    "    \"TGLANG_LANGUAGE_TYPESCRIPT\": \"typescript\",\n",
    "    \"TGLANG_LANGUAGE_UNREALSCRIPT\": \"unrealscript\",\n",
    "    \"TGLANG_LANGUAGE_VALA\": \"vala\",\n",
    "    \"TGLANG_LANGUAGE_VBSCRIPT\": \"TGLANG_LANGUAGE_VBSCRIPT\",\n",
    "    \"TGLANG_LANGUAGE_VERILOG\": \"verilog\",\n",
    "    \"TGLANG_LANGUAGE_VISUAL_BASIC\": \"visual-basic\",\n",
    "    \"TGLANG_LANGUAGE_WOLFRAM\": \"mathematica\",\n",
    "    \"TGLANG_LANGUAGE_XML\": \"xml\",\n",
    "    \"TGLANG_LANGUAGE_YAML\": \"yaml\",\n",
    "}\n",
    "languages = list(language_mapping.values())\n",
    "num_languages = len(languages)\n",
    "print(num_languages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_to_extract = [\n",
    "\t\"TGLANG_LANGUAGE_OTHER\",\n",
    "    \"TGLANG_LANGUAGE_C\",\n",
    "    \"TGLANG_LANGUAGE_CPLUSPLUS\",\n",
    "    \"TGLANG_LANGUAGE_CSHARP\",\n",
    "    \"TGLANG_LANGUAGE_CSS\",\n",
    "    \"TGLANG_LANGUAGE_DART\",\n",
    "    \"TGLANG_LANGUAGE_DOCKER\",\n",
    "    \"TGLANG_LANGUAGE_FUNC\",\n",
    "    \"TGLANG_LANGUAGE_GO\",\n",
    "    \"TGLANG_LANGUAGE_HTML\",\n",
    "    \"TGLANG_LANGUAGE_JAVA\",\n",
    "    \"TGLANG_LANGUAGE_JAVASCRIPT\",\n",
    "    \"TGLANG_LANGUAGE_JSON\",\n",
    "    \"TGLANG_LANGUAGE_KOTLIN\",\n",
    "    \"TGLANG_LANGUAGE_LUA\",\n",
    "    \"TGLANG_LANGUAGE_NGINX\",\n",
    "    \"TGLANG_LANGUAGE_OBJECTIVE_C\",\n",
    "    \"TGLANG_LANGUAGE_PHP\",\n",
    "    \"TGLANG_LANGUAGE_POWERSHELL\",\n",
    "    \"TGLANG_LANGUAGE_PYTHON\",\n",
    "    \"TGLANG_LANGUAGE_RUBY\",\n",
    "    \"TGLANG_LANGUAGE_RUST\",\n",
    "    \"TGLANG_LANGUAGE_SHELL\",\n",
    "    \"TGLANG_LANGUAGE_SOLIDITY\",\n",
    "    \"TGLANG_LANGUAGE_SQL\",\n",
    "    \"TGLANG_LANGUAGE_SWIFT\",\n",
    "    \"TGLANG_LANGUAGE_TL\",\n",
    "    \"TGLANG_LANGUAGE_TYPESCRIPT\",\n",
    "    \"TGLANG_LANGUAGE_XML\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_language_mapping = {key: language_mapping[key] for key in languages_to_extract if key in language_mapping}\n",
    "\n",
    "new_language_mapping_list = list(new_language_mapping.keys())\n",
    "\n",
    "not_existed_indexes = []\n",
    "for i, el in enumerate(language_mapping.keys()):\n",
    "\tif not el in new_language_mapping_list:\n",
    "\t\tnot_existed_indexes.append(i)\n",
    "\n",
    "num_languages_new = len(languages_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other', 'c', 'c++', 'c-sharp', 'css', 'dart', 'dockerfile', 'FUNC', 'go', 'html', 'java', 'javascript', 'TGLANG_LANGUAGE_JSON', 'kotlin', 'lua', 'nginx', 'TGLANG_LANGUAGE_OBJECTIVE_C', 'php', 'powershell', 'python', 'ruby', 'rust', 'shell', 'solidity', 'sql', 'swift', 'thrift', 'typescript', 'xml']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# Create the new_lang_list using a list comprehension\n",
    "new_lang_list = [language_mapping[lang_key] for lang_key in languages_to_extract]\n",
    "\n",
    "# Print the resulting list\n",
    "print(new_lang_list)\n",
    "num_languages = len(new_lang_list)\n",
    "print(num_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip raw_data_v2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save list of files to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Languages: 29\n",
      "Language to Files Mapping saved to ./raw_data_v2/language_files_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the base directory where you want to search for files\n",
    "base_directory = \"./raw_data_v2\"\n",
    "\n",
    "# Initialize a dictionary to store the mapping of language to files\n",
    "language_files_mapping = {}\n",
    "\n",
    "# Loop through each language in the language_mapping dictionary\n",
    "for language_code, language_name in language_mapping.items():\n",
    "\t# Define the directory for the current language based on its name\n",
    "\tlanguage_directory = os.path.join(base_directory, language_name)\n",
    "\n",
    "\t# Check if the directory exists\n",
    "\tif os.path.exists(language_directory):\n",
    "\t\t# Get a list of files in the directory\n",
    "\t\tfile_list = os.listdir(language_directory)\n",
    "\n",
    "\t\t# Add the list of files to the mapping dictionary\n",
    "\t\tlanguage_files_mapping[language_name] = file_list\n",
    "\n",
    "# Define the output file path where you want to save the mapping\n",
    "output_file_path = \"./raw_data_v2/language_files_mapping.json\"\n",
    "\n",
    "# Save the language to files mapping as a JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "\tjson.dump(language_files_mapping, output_file, indent=4)\n",
    "\n",
    "# Print the total number of languages and the saved mapping\n",
    "print(\"Total Number of Languages:\", num_languages)\n",
    "print(\"Language to Files Mapping saved to\", output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print dataset summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: other, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_1S_ENTERPRISE, File Count: 10\n",
      "Language: abap, File Count: 10\n",
      "Language: actionscript, File Count: 10\n",
      "Language: ada, File Count: 10\n",
      "Language: groovy, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_APEX, File Count: 10\n",
      "Language: applescript, File Count: 10\n",
      "Language: asp, File Count: 10\n",
      "Language: assembly, File Count: 10\n",
      "Language: autohotkey, File Count: 10\n",
      "Language: awk, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_BASIC, File Count: 10\n",
      "Language: batchfile, File Count: 10\n",
      "Language: bison, File Count: 10\n",
      "Language: c, File Count: 10\n",
      "Language: clojure, File Count: 10\n",
      "Language: cmake, File Count: 10\n",
      "Language: cobol, File Count: 10\n",
      "Language: coffeescript, File Count: 10\n",
      "Language: common-lisp, File Count: 10\n",
      "Language: c++, File Count: 10\n",
      "Language: crystal, File Count: 10\n",
      "Language: c-sharp, File Count: 10\n",
      "Language: css, File Count: 10\n",
      "Language: csv, File Count: 10\n",
      "Language: d, File Count: 10\n",
      "Language: dart, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_DELPHI, File Count: 10\n",
      "Language: dockerfile, File Count: 10\n",
      "Language: elixir, File Count: 10\n",
      "Language: elm, File Count: 10\n",
      "Language: erlang, File Count: 10\n",
      "Language: FIFT, File Count: 10\n",
      "Language: forth, File Count: 10\n",
      "Language: fortran, File Count: 10\n",
      "Language: f-sharp, File Count: 10\n",
      "Language: FUNC, File Count: 10\n",
      "Language: gams, File Count: 10\n",
      "Language: go, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_GRADLE, File Count: 10\n",
      "Language: graphql, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_HACK, File Count: 10\n",
      "Language: haskell, File Count: 10\n",
      "Language: html, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_ICON, File Count: 10\n",
      "Language: idl, File Count: 10\n",
      "Language: ini, File Count: 10\n",
      "Language: java, File Count: 10\n",
      "Language: javascript, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_JSON, File Count: 10\n",
      "Language: julia, File Count: 10\n",
      "Language: kmn_dataset, File Count: 10\n",
      "Language: kotlin, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_LATEX, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_LISP, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_LOGO, File Count: 10\n",
      "Language: lua, File Count: 10\n",
      "Language: makefile, File Count: 10\n",
      "Language: markdown, File Count: 10\n",
      "Language: matlab, File Count: 10\n",
      "Language: nginx, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_NIM, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_OBJECTIVE_C, File Count: 10\n",
      "Language: ocaml, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_OPENEDGE_ABL, File Count: 10\n",
      "Language: pascal, File Count: 10\n",
      "Language: perl, File Count: 10\n",
      "Language: php, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_PL_SQL, File Count: 10\n",
      "Language: powershell, File Count: 10\n",
      "Language: prolog, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_PROTOBUF, File Count: 10\n",
      "Language: python, File Count: 10\n",
      "Language: qml, File Count: 10\n",
      "Language: r, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_RAKU, File Count: 10\n",
      "Language: regexp, File Count: 10\n",
      "Language: ruby, File Count: 10\n",
      "Language: rust, File Count: 10\n",
      "Language: sas, File Count: 10\n",
      "Language: scala, File Count: 10\n",
      "Language: scheme, File Count: 10\n",
      "Language: shell, File Count: 10\n",
      "Language: smalltalk, File Count: 10\n",
      "Language: solidity, File Count: 10\n",
      "Language: sql, File Count: 10\n",
      "Language: swift, File Count: 10\n",
      "Language: tcl, File Count: 10\n",
      "Language: textile, File Count: 10\n",
      "Language: thrift, File Count: 10\n",
      "Language: typescript, File Count: 10\n",
      "Language: unrealscript, File Count: 10\n",
      "Language: vala, File Count: 10\n",
      "Language: TGLANG_LANGUAGE_VBSCRIPT, File Count: 10\n",
      "Language: verilog, File Count: 10\n",
      "Language: visual-basic, File Count: 10\n",
      "Language: mathematica, File Count: 10\n",
      "Language: xml, File Count: 10\n",
      "Language: yaml, File Count: 10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to the JSON file containing the mapping\n",
    "input_file_path = \"./raw_data_v2/language_files_mapping.json\"\n",
    "\n",
    "# Initialize an empty list to store language-count pairs\n",
    "language_counts = []\n",
    "\n",
    "# Load the mapping from the JSON file\n",
    "with open(input_file_path, \"r\") as input_file:\n",
    "    loaded_language_files_mapping = json.load(input_file)\n",
    "\n",
    "# Calculate and store the counts of files for each language\n",
    "for language, file_list in loaded_language_files_mapping.items():\n",
    "    language_counts.append((language, len(file_list)))\n",
    "\n",
    "# Sort the list of language-count pairs by count in descending order\n",
    "language_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted counts\n",
    "for language, count in language_counts:\n",
    "    print(f\"Language: {language}, File Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate files to train and validate and save to jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to ./raw_data_v2/train_language_files_mapping.json\n",
      "Validate data saved to ./raw_data_v2/validate_language_files_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Define the path to the JSON file containing the mapping\n",
    "input_file_path = \"./raw_data_v2/language_files_mapping.json\"\n",
    "\n",
    "# Define the paths for the output train and validation JSON files\n",
    "train_output_path = \"./raw_data_v2/train_language_files_mapping.json\"\n",
    "validate_output_path = \"./raw_data_v2/validate_language_files_mapping.json\"\n",
    "\n",
    "# Define the split ratio (90% train, 10% validate)\n",
    "split_ratio = 0.9\n",
    "\n",
    "# Initialize dictionaries for train and validate data\n",
    "train_data = {}\n",
    "validate_data = {}\n",
    "\n",
    "# Load the mapping from the JSON file\n",
    "with open(input_file_path, \"r\") as input_file:\n",
    "\tloaded_language_files_mapping = json.load(input_file)\n",
    "\n",
    "# Split the data into train and validate for each language\n",
    "for language, file_list in loaded_language_files_mapping.items():\n",
    "\t# Shuffle the file list randomly\n",
    "\trandom.shuffle(file_list)\n",
    "\t\n",
    "\t# Calculate the split index based on the split_ratio\n",
    "\tsplit_index = int(len(file_list) * split_ratio)\n",
    "\t\n",
    "\t# Split the files into train and validate lists\n",
    "\ttrain_files = file_list[:split_index]\n",
    "\tvalidate_files = file_list[split_index:]\n",
    "\t\n",
    "\t# Add the language and file lists to the train and validate dictionaries\n",
    "\ttrain_data[language] = train_files\n",
    "\tvalidate_data[language] = validate_files\n",
    "\n",
    "# Save the train and validate data as separate JSON files\n",
    "with open(train_output_path, \"w\") as train_output_file:\n",
    "\tjson.dump(train_data, train_output_file, indent=4)\n",
    "\n",
    "with open(validate_output_path, \"w\") as validate_output_file:\n",
    "\tjson.dump(validate_data, validate_output_file, indent=4)\n",
    "\n",
    "# Print confirmation messages\n",
    "print(f\"Train data saved to {train_output_path}\")\n",
    "print(f\"Validate data saved to {validate_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read list of files for train and list for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the paths to the train and validate JSON files\n",
    "train_output_path = \"./raw_data_v2/train_language_files_mapping.json\"\n",
    "validate_output_path = \"./raw_data_v2/validate_language_files_mapping.json\"\n",
    "\n",
    "# Initialize dictionaries for train and validate data\n",
    "train_data = {}\n",
    "validate_data = {}\n",
    "\n",
    "# Load the train data from the JSON file\n",
    "with open(train_output_path, \"r\") as train_input_file:\n",
    "    train_data = json.load(train_input_file)\n",
    "\n",
    "# Load the validate data from the JSON file\n",
    "with open(validate_output_path, \"r\") as validate_input_file:\n",
    "    validate_data = json.load(validate_input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "SPECIAL_TOKENS = {'pad_token':'<pad>'}\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Methods to prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "\n",
    "datafolder = \"./raw_data_v2\"\n",
    "minimum_length = 10\n",
    "text_len = 256\n",
    "num_threads = 1\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "\t\"\"\" 1-hot encodes a tensor \"\"\"\n",
    "\treturn np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def read_file_with_retry(file_path, max_length=None):\n",
    "\ttry:\n",
    "\t\twith open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "\t\t\tcontent = file.read(max_length)\n",
    "\t\treturn content\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\ttry:\n",
    "\t\t\twith open(file_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "\t\t\t\tcontent = file.read(max_length)\n",
    "\t\t\treturn content\n",
    "\t\texcept UnicodeDecodeError:\n",
    "\t\t\ttry:\n",
    "\t\t\t\twith open(file_path, \"r\", encoding=\"iso-8859-1\") as file:\n",
    "\t\t\t\t\tcontent = file.read(max_length)\n",
    "\t\t\t\treturn content\n",
    "\t\t\texcept UnicodeDecodeError:\n",
    "\t\t\t\tprint(f\"Error reading file '{file_path}'. Trying another file...\")\n",
    "\t\t\t\treturn None\n",
    "\n",
    "def tokenize_and_save_fragments(data, tokenizer, output_folder, num_batches, text_len, num_threads):\n",
    "\tos.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\tlast_index = 0\n",
    "\twhile os.path.exists(os.path.join(batches_folder, f\"batch_{last_index + 1}.pkl\")):\n",
    "\t\tlast_index += 1\n",
    "\n",
    "\tprint(last_index, num_batches)\n",
    "\t# def tokenize_and_save_batch(batch_index):\n",
    "\tfor batch_index in range(last_index, num_batches):\n",
    "\t\tcontents = []\n",
    "\t\tbatch_data = []  # List to hold the batch data\n",
    "\t\tbatch_label_indexes = []  # List to hold the index (key of the value)\n",
    "\t\tbatch_labels_categorical = []  # List to hold one-hot encoded labels\n",
    "\t\t# for i, (language, file_list) in enumerate(data.items()):\n",
    "\t\tfor i, (language, folder_name) in enumerate(language_mapping.items()):\n",
    "\t\t\t# if not folder_name in data or len(data[folder_name]) == 0:\n",
    "\t\t\t# \tcontents.append(\"\")\n",
    "\t\t\t# \tcontinue\n",
    "\t\t\tfile_list = data[folder_name]\n",
    "\t\t\tfile_name = file_list[batch_index % len(file_list)]\n",
    "\t\t\t\n",
    "\t\t\t# Read the content of the file\n",
    "\t\t\tcontent = read_file_with_retry(os.path.join(datafolder, folder_name, file_name), 10000)\n",
    "\t\t\t\n",
    "\t\t\t# print(content)\n",
    "\t\t\t\n",
    "\t\t\t# Tokenize the content\n",
    "\t\t\tcontents.append(content)\n",
    "\t\t\t\n",
    "\t\ttokens = tokenizer(contents, return_tensors=\"pt\", padding=True,  max_length=5000)\n",
    "\n",
    "\t\t# for i, (language, file_list) in enumerate(data.items()):\n",
    "\t\tfor i, (language, folder_name) in enumerate(language_mapping.items()):\n",
    "\t\t\t# if not folder_name in data or len(data[folder_name]) == 0:\n",
    "\t\t\t# \tbatch_data.append(torch.tensor([[0 for i in range(text_len)]]))  # Add content to the batch\n",
    "\t\t\t# \tbatch_labels_categorical.append(torch.tensor([0 for i in range(29)]))\n",
    "\t\t\t# \tbatch_label_indexes.append(-1)  # Add the index (key) to the batch\n",
    "\t\t\t# \tcontinue\n",
    "\t\t\tfile_list = data[folder_name]\n",
    "\t\t\ttoken_ids = tokens[\"input_ids\"][i]\n",
    "\n",
    "\t\t\t# Calculate text_len based on the first pad position\n",
    "\t\t\tfirst_pad_position = (token_ids == tokenizer.pad_token_id).nonzero()\n",
    "\t\t\tif len(first_pad_position) > 0:\n",
    "\t\t\t\ttoken_ids_len = first_pad_position[0, 0].item()\n",
    "\t\t\telse:\n",
    "\t\t\t\ttoken_ids_len = len(token_ids)\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tfile_name = file_list[batch_index % len(file_list)]\n",
    "\t\t\t# print(f\"file:{folder_name}/{file_name}\")\n",
    "\t\t\t# print(f\"len:{token_ids_len}\")\n",
    "\t\t\t\n",
    "\t\t\tmin_start_idx = 0\n",
    "\t\t\tmax_start_idx = max(0, token_ids_len - minimum_length)\n",
    "\t\t\tstart_idx = np.random.randint(min_start_idx, max_start_idx) if max_start_idx > 0 else 0\n",
    "\t\t\t# print(f\"start_idx: {min_start_idx} < {start_idx} < {max_start_idx}\")\n",
    "\n",
    "\t\t\tmin_end_idx = min(token_ids_len, start_idx + minimum_length)\n",
    "\t\t\tmax_end_idx = min(token_ids_len, start_idx + text_len)\n",
    "\t\t\tend_idx = token_ids_len if min_end_idx == max_end_idx else np.random.randint(min_end_idx, max_end_idx)\n",
    "\t\t\tresult_text_len = end_idx - start_idx\n",
    "\t\t\t# print(f\"end_idx: {min_end_idx} < {end_idx} < {max_end_idx} (len: {result_text_len})\")\n",
    "\n",
    "\n",
    "\t\t\ttruncated_token_ids = token_ids[start_idx:end_idx]\n",
    "\t\t\t# print(f\"truncated_token_ids {truncated_token_ids.shape}\")\n",
    "\t\t\tpadding_length = text_len - len(truncated_token_ids)\n",
    "\t\t\tif padding_length > 0:\n",
    "\t\t\t\tpadded_token_ids = torch.cat([truncated_token_ids, torch.full((padding_length,), fill_value=tokenizer.pad_token_id, dtype=torch.long)], dim=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpadded_token_ids = truncated_token_ids\n",
    "\t\t\t# print(f\"padded_token_ids {padded_token_ids.shape}\")\n",
    "\n",
    "\t\t\t# detokenized_text = tokenizer.decode(padded_token_ids)\n",
    "\t\t\t# print(detokenized_text)\n",
    "\t\t\n",
    "\t\t\tlabel_categorical = to_categorical(i, len(data))# num_languages)\n",
    "\n",
    "\t\t\tbatch_data.append(padded_token_ids.unsqueeze(0))  # Add content to the batch\n",
    "\t\t\tbatch_labels_categorical.append(label_categorical)\n",
    "\t\t\tbatch_label_indexes.append(i)  # Add the index (key) to the batch\n",
    "\n",
    "\n",
    "\t\tbatch_data = torch.cat(batch_data, dim=0)\n",
    "\t\tbatch_labels_categorical = torch.tensor(batch_labels_categorical, dtype=torch.float32)\n",
    "\t\tbatch_label_indexes = torch.tensor(batch_label_indexes)\n",
    "\t\t\n",
    "\t\tbatch_filename = os.path.join(output_folder, f\"batch_{batch_index+1}.pkl\")\n",
    "\n",
    "\t\t# print(f\"Batch {batch_index+1} batch_data Shape: {batch_data.shape}\")\n",
    "\t\t# print(f\"Batch {batch_index+1} batch_label Shape: {batch_labels_categorical.shape}\")\n",
    "\t\t# print(f\"Batch {batch_index+1} batch_label_indexes Shape: {batch_label_indexes.shape}\")\n",
    "\t\t\n",
    "\t\tprint(f\"save {batch_filename} (len: {end_idx - start_idx})\")\n",
    "\t\twith open(batch_filename, \"wb\") as f:\n",
    "\t\t\tpickle.dump((batch_data, batch_labels_categorical, batch_label_indexes), f)\n",
    "\n",
    "\t# with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "\t# \tfutures = []\n",
    "\t# \tfor batch_index in range(last_index, num_batches):\n",
    "\t# \t\tfutures.append(executor.submit(tokenize_and_save_batch, batch_index))\n",
    "\n",
    "\t# \tfor future in concurrent.futures.as_completed(futures):\n",
    "\t# \t\tfuture.result()\n",
    "\n",
    "# Usage example:\n",
    "# train_data or validate_data should be loaded and structured properly\n",
    "# tokenize_and_save_fragments(data_to_write, tokenizer, batches_folder, num_batches, text_len, num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving train batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vadimmakarov/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/var/folders/9z/q_wbkn9n3rx2yphg29mvstph0000gn/T/ipykernel_43622/1278178365.py:123: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  batch_labels_categorical = torch.tensor(batch_labels_categorical, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save ./dataset_v4o_train/batch_1.pkl (len: 48)\n",
      "save ./dataset_v4o_train/batch_2.pkl (len: 29)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batches_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./dataset_v4o_train\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39m50000\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenize_and_save_fragments(train_data, tokenizer, batches_folder, num_batches, text_len, num_threads)\n",
      "\u001b[1;32m/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \t\u001b[39m# print(content)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \t\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \t\u001b[39m# Tokenize the content\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \tcontents\u001b[39m.\u001b[39mappend(content)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer(contents, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  max_length\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# for i, (language, file_list) in enumerate(data.items()):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (language, folder_name) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(language_mapping\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \t\u001b[39m# if not folder_name in data or len(data[folder_name]) == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \t\u001b[39m# \tbatch_data.append(torch.tensor([[0 for i in range(text_len)]]))  # Add content to the batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \t\u001b[39m# \tbatch_labels_categorical.append(torch.tensor([0 for i in range(29)]))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \t\u001b[39m# \tbatch_label_indexes.append(-1)  # Add the index (key) to the batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X30sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \t\u001b[39m# \tcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[1;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2887\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2888\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2889\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2890\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2891\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2892\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2893\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2894\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2895\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2896\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2897\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2898\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2899\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2900\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2901\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m )\n\u001b[0;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3093\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/models/gpt2/tokenization_gpt2_fast.py:162\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    158\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_batch_encode_plus(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    745\u001b[0m     value \u001b[39m=\u001b[39m [value]\n\u001b[1;32m    747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    750\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(value[\u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batches_folder = \"./dataset_v4o_train\"\n",
    "num_batches = 50000\n",
    "tokenize_and_save_fragments(train_data, tokenizer, batches_folder, num_batches, text_len, num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving validate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5000\n",
      "save ./dataset_v4o_validate/batch_1.pkl (len: 93)\n",
      "save ./dataset_v4o_validate/batch_2.pkl (len: 204)\n",
      "save ./dataset_v4o_validate/batch_3.pkl (len: 109)\n",
      "save ./dataset_v4o_validate/batch_4.pkl (len: 17)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batches_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./dataset_v4o_validate\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenize_and_save_fragments(validate_data, tokenizer, batches_folder, num_batches, text_len, num_threads)\n",
      "\u001b[1;32m/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \t\u001b[39m# print(content)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \t\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \t\u001b[39m# Tokenize the content\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \tcontents\u001b[39m.\u001b[39mappend(content)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m tokens \u001b[39m=\u001b[39m tokenizer(contents, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  max_length\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# for i, (language, file_list) in enumerate(data.items()):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (language, folder_name) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(language_mapping\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \t\u001b[39m# if not folder_name in data or len(data[folder_name]) == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \t\u001b[39m# \tbatch_data.append(torch.tensor([[0 for i in range(text_len)]]))  # Add content to the batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \t\u001b[39m# \tbatch_labels_categorical.append(torch.tensor([0 for i in range(29)]))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \t\u001b[39m# \tbatch_label_indexes.append(-1)  # Add the index (key) to the batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vadimmakarov/Documents/Work_home/submission/train/dataset-prepare.ipynb#X25sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \t\u001b[39m# \tcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[1;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2887\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2888\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2889\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2890\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2891\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2892\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2893\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2894\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2895\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2896\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2897\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2898\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2899\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2900\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2901\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m )\n\u001b[0;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3093\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/models/gpt2/tokenization_gpt2_fast.py:162\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m is_split_into_words \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_split_into_words\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    157\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_prefix_space \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    158\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou need to instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with add_prefix_space=True \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mto use it with pretokenized inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_batch_encode_plus(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    745\u001b[0m     value \u001b[39m=\u001b[39m [value]\n\u001b[1;32m    747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    750\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/Documents/Work_home/cp-gpt/transformers/src/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(value[\u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batches_folder = \"./dataset_v4o_validate\"\n",
    "num_batches = 5000\n",
    "tokenize_and_save_fragments(validate_data, tokenizer, batches_folder, num_batches, text_len, num_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
